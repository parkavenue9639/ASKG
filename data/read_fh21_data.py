import torch
from torch.utils.data import Dataset

import os
import pickle as pkl
import numpy as np
from sklearn.model_selection import train_test_split


class Fh21Dataset(Dataset):
    def __init__(self, opt, split='train'):
        
        # ä½¿ç”¨ç»å¯¹è·¯å¾„
        self.data_dir = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), 
                                    'data/processed_fh21_precise_tag')
        self.num_medterm = opt.num_medterm  # æŒ‡å®šåŒ»å­¦æœ¯è¯­çš„æ•°é‡

        # è¯»å–pklæ–‡ä»¶ï¼Œè¯¥æ–‡ä»¶æ˜¯ä¸€ä¸ªpickelåºåˆ—åŒ–çš„æ–‡ä»¶ï¼Œå­˜å‚¨äº†æ•°æ®é›†çš„ä¸»è¦å†…å®¹
        with open(os.path.join(self.data_dir, 'fh21.pkl'), 'rb') as f:
            self.data = pkl.load(f)

        # è¯¥æ–‡ä»¶æ˜¯ä¸€ä¸ªword-to-idè¯å…¸ï¼Œå°†å•è¯æ˜ å°„åˆ°å”¯ä¸€çš„æ•´æ•°id
        with open(os.path.join(self.data_dir, 'word2idw.pkl'), 'rb') as f:
            self.word2idw = pkl.load(f)

        # åˆ›å»ºåå‘è¯å…¸ï¼Œç”¨äºå°†idè½¬æ¢ä¸ºå•è¯
        self.idw2word = {v: k for k, v in self.word2idw.items()}

        # è·å–è¯æ±‡å¤§å°
        self.vocab_size = len(self.word2idw)

        train_data, test_data = train_test_split(self.data, test_size=0.3, random_state=42)
        valid_data, test_data = train_test_split(test_data, test_size=0.5, random_state=42)
        if split == 'train':
            self.data = train_data
        elif split == 'valid':
            self.data = valid_data
        elif split == 'test':
            self.data = test_data
        else:
            raise ValueError('split must be train or valid or test')

        self.check_data()

        print("Fh21Dataset {} init complete, len : {}".format(split, len(self.data)))

    def __getitem__(self, index):
        # ç”¨äºè·å–indexä½ç½®çš„æ•°æ®ï¼Œè¿”å›ç´¢å¼•ã€æ‘˜è¦æ•°æ®ã€æ‘˜è¦æ ‡ç­¾ã€åŒ»å­¦æœ¯è¯­æ ‡ç­¾
        ix = self.data[index][0]
        abstracts = self.data[index][1]
        abstracts = np.array(abstracts)

        abstracts_labels = self.data[index][2]
        abstracts_labels = np.array(abstracts_labels)

        # è½¬æ¢ä¸ºtorchå¼ é‡
        abstracts = torch.from_numpy(abstracts).long()
        abstracts_labels = torch.from_numpy(abstracts_labels).long()

        # å¤„ç†åŒ»å­¦æœ¯è¯­æ ‡ç­¾
        med_terms_labels = np.zeros(229)
        med_terms = self.data[index][3]
        for med_term in med_terms:
            # med_term_labels[med_term] = 1
            if med_term < self.num_medterm:
                # # ç‹¬çƒ­å‘é‡ç¼–ç å‘é‡ï¼Œè¡¨ç¤ºè¯¥æ‘˜è¦åŒ…å«å“ªäº›åŒ»å­¦æœ¯è¯­ã€‚
                med_terms_labels[med_term] = 1  # è¡¨ç¤ºè¯¥æ‘˜è¦è®¾è®¡med_termè¿™ä¸ªåŒ»å­¦æœ¯è¯­

        # æ•°æ®ç´¢å¼•ã€è½¬æ¢ä¸ºtorchå¼ é‡çš„æ‘˜è¦ã€æ‘˜è¦çš„æ ‡ç­¾ã€åŒ»å­¦æœ¯è¯­çš„ç‹¬çƒ­ç¼–ç é‡
        return ix, abstracts, abstracts_labels, torch.FloatTensor(med_terms_labels)

    def __len__(self):
        #  æ•°æ®é›†é•¿åº¦
        return len(self.data)

    def check_data(self):
        print("Fh21Dataset Data ç±»å‹ï¼š", type(self.data))  # list
        if isinstance(self.data, list):
            print("Fh21Dataset Data é•¿åº¦ï¼š", len(self.data))  # 29058
            print("Fh21Dataset Data ç¤ºä¾‹ï¼ˆå‰ 1 ä¸ªå…ƒç´ ï¼‰ï¼š", self.data[0])

        print("Fh21Dataset word2idw é•¿åº¦ï¼š", len(self.word2idw))
        print("Fh21Dataset idw2word é•¿åº¦ï¼š", len(self.idw2word))
        print("è¯è¡¨ç¤ºä¾‹ï¼š", dict(list(self.idw2word.items())[:5]))

        print("\nğŸ” è§£ç å‰ 3 æ¡æ•°æ®ï¼ˆabstractsï¼‰ï¼š")
        for i, sample in enumerate(self.data[:3]):
            image_id, abstract_ids, label_ids, medterm_ids = sample
            decoded = self.decode_ids_with_transformer_logic([abstract_ids])[0]
            print(f"\n Sample {i + 1}: Image ID = {image_id}")
            print("  Raw IDs      :", abstract_ids)
            print("  Decoded Text :", decoded)

    def decode_ids_with_transformer_logic(self, batch_abstract_ids):
        """
        æ¨¡æ‹Ÿæ¨¡å‹è§£ç é€»è¾‘ï¼Œå°†ä¸€æ‰¹ abstract idsï¼ˆList[List[int]]ï¼‰è½¬æ¢ä¸ºå­—ç¬¦ä¸²
        """
        decode_list = []
        for abstract_ids in batch_abstract_ids:
            words = []
            for token_id in abstract_ids:
                if token_id < 0:
                    continue
                token = self.idw2word.get(token_id, '<UNK>')
                if token in ['<BOS>', '<BLANK>', '<UNK>']:
                    continue
                if token == '<EOS>':
                    break
                words.append(token)
            decode_list.append(' '.join(words))
        return decode_list


